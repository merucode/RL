{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/merucode/RL/blob/32-Study-Udemy-Policy_gradient/4_proximal_policy_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1fDTR8FNJVX"
      },
      "source": [
        "# Proximal Policy Optimization (PPO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uvj2rXR2f1Kw"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y xvfb\n",
        "\n",
        "!pip install gym==0.23.1 \\\n",
        "    pytorch-lightning==1.6 \\\n",
        "    pyvirtualdisplay\n",
        "\n",
        "!pip install -U brax==0.0.12 jax==0.3.14 jaxlib==0.3.14+cuda11.cudnn82 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOSJl-X7zvs4"
      },
      "source": [
        "#### Setup virtual display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-Z6takfzqGk"
      },
      "outputs": [],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display(visible=False, size=(1400, 900)).start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz8DLleGz_TF"
      },
      "source": [
        "#### Import the necessary code libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP5t6U7-nYoc"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import gym\n",
        "import matplotlib\n",
        "import functools\n",
        "import itertools\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import IterableDataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from torch.distributions import Normal\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "\n",
        "import brax\n",
        "from brax import envs\n",
        "from brax.envs import to_torch\n",
        "from brax.io import html\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "num_gpus = torch.cuda.device_count()\n",
        "\n",
        "v = torch.ones(1, device='cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLpKsS8qPzsN"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def create_video(env, episode_length, policy=None):\n",
        "  qp_array = []\n",
        "  state = env.reset()\n",
        "  for i in range(episode_length):\n",
        "    if policy:\n",
        "      loc, scale = policy(state)\n",
        "      sample = torch.normal(loc, scale)\n",
        "      action = torch.tanh(sample)\n",
        "    else:\n",
        "      action = env.action_space.sample()\n",
        "    state, _, _, _ = env.step(action)\n",
        "    qp_array.append(env.unwrapped._state.qp)\n",
        "  return HTML(html.render(env.unwrapped._env.sys, qp_array))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_agent(env, episode_length, policy, episodes=10):\n",
        "\n",
        "  ep_returns = []\n",
        "  for ep in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    ep_ret = 0.0\n",
        "\n",
        "    while not done:\n",
        "      loc, scale = policy(state)\n",
        "      sample = torch.normal(loc, scale)\n",
        "      action = torch.tanh(sample)\n",
        "      state, reward, done, info = env.step(action)\n",
        "      ep_ret += reward.item()\n",
        "\n",
        "    ep_returns.append(ep_ret)\n",
        "\n",
        "  return sum(ep_returns) / episodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSsBrSxYIkh8"
      },
      "source": [
        "#### Create the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hht5L2bjiNw"
      },
      "outputs": [],
      "source": [
        "class GradientPolicy(nn.Module):\n",
        "\n",
        "  def __init__(self, in_features, out_dims, hidden_size=128):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(in_features, hidden_size)\n",
        "    self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.fc_mu = nn.Linear(hidden_size, out_dims)\n",
        "    self.fc_std = nn.Linear(hidden_size, out_dims)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    loc = self.fc_mu(x)\n",
        "    loc = torch.tanh(loc)\n",
        "    scale = self.fc_std(x)\n",
        "    scale = F.softplus(scale) + 0.001\n",
        "    return loc, scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTOQuwEzIolu"
      },
      "source": [
        "#### Create the value network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtDyLSLCjiP-"
      },
      "outputs": [],
      "source": [
        "class ValueNet(nn.Module):\n",
        "\n",
        "  def __init__(self, in_features, hidden_size=128):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(in_features, hidden_size)\n",
        "    self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.fc3 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2EMZSjMIxgr"
      },
      "source": [
        "#### Create the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-AI3Jbkr4sA"
      },
      "outputs": [],
      "source": [
        "class RunningMeanStd:\n",
        "    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n",
        "    def __init__(self, epsilon=1e-4, shape=()):\n",
        "        self.mean = torch.zeros(shape, dtype=torch.float32).to(device)\n",
        "        self.var = torch.ones(shape, dtype=torch.float32).to(device)\n",
        "        self.count = epsilon\n",
        "\n",
        "    def update(self, x):\n",
        "        batch_mean = torch.mean(x, dim=0)\n",
        "        batch_var = torch.var(x, dim=0)\n",
        "        batch_count = x.shape[0]\n",
        "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
        "\n",
        "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
        "        self.mean, self.var, self.count = update_mean_var_count_from_moments(\n",
        "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count\n",
        "        )\n",
        "\n",
        "\n",
        "def update_mean_var_count_from_moments(\n",
        "    mean, var, count, batch_mean, batch_var, batch_count\n",
        "):\n",
        "    delta = batch_mean - mean\n",
        "    tot_count = count + batch_count\n",
        "\n",
        "    new_mean = mean + delta * batch_count / tot_count\n",
        "    m_a = var * count\n",
        "    m_b = batch_var * batch_count\n",
        "    M2 = m_a + m_b + torch.square(delta) * count * batch_count / tot_count\n",
        "    new_var = M2 / tot_count\n",
        "    new_count = tot_count\n",
        "\n",
        "    return new_mean, new_var, new_count\n",
        "\n",
        "\n",
        "class NormalizeObservation(gym.core.Wrapper):\n",
        "\n",
        "    def __init__(self, env, epsilon=1e-8):\n",
        "        super().__init__(env)\n",
        "        self.num_envs = getattr(env, \"num_envs\", 1)\n",
        "        self.obs_rms = RunningMeanStd(shape=self.observation_space.shape[-1])\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rews, dones, infos = self.env.step(action)\n",
        "        obs = self.normalize(obs)\n",
        "        return obs, rews, dones, infos\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return_info = kwargs.get(\"return_info\", False)\n",
        "        if return_info:\n",
        "            obs, info = self.env.reset(**kwargs)\n",
        "        else:\n",
        "            obs = self.env.reset(**kwargs)\n",
        "        obs = self.normalize(obs)\n",
        "        if not return_info:\n",
        "            return obs\n",
        "        else:\n",
        "            return obs, info\n",
        "\n",
        "    def normalize(self, obs):\n",
        "        self.obs_rms.update(obs)\n",
        "        return (obs - self.obs_rms.mean) / torch.sqrt(self.obs_rms.var + self.epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpD2i_ZSHRmN"
      },
      "outputs": [],
      "source": [
        "entry_point = functools.partial(envs.create_gym_env, env_name='ant')\n",
        "gym.register('brax-ant-v0', entry_point=entry_point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX4y_05HHRpD"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"brax-ant-v0\", episode_length=1000)\n",
        "env = to_torch.JaxToTorchWrapper(env, device=device)\n",
        "create_video(env, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dl9rXj2Sun2"
      },
      "outputs": [],
      "source": [
        "def create_env(env_name, num_envs=256, episode_length=1000):\n",
        "  env = gym.make(env_name, batch_size=num_envs, episode_length=episode_length)\n",
        "  env = to_torch.JaxToTorchWrapper(env, device=device)\n",
        "  env = NormalizeObservation(env)\n",
        "  return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEhh8tbITjvt"
      },
      "outputs": [],
      "source": [
        "env = create_env('brax-ant-v0', num_envs=10)\n",
        "obs = env.reset()\n",
        "print(\"Num envs: \", obs.shape[0], \"Obs dimentions: \", obs.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WPLsdLwTjyZ"
      },
      "outputs": [],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_LlyDyuWFcX"
      },
      "outputs": [],
      "source": [
        "obs, reward, done, info = env.step(env.action_space.sample())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vl4GvpJ4Yvl9"
      },
      "outputs": [],
      "source": [
        "info.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hI3qJPwI4Xo"
      },
      "source": [
        "#### Create the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEJnLgAnV_iR"
      },
      "outputs": [],
      "source": [
        "class RLDataset(IterableDataset):\n",
        "\n",
        "  def __init__(self, env, policy, samples_per_epoch, epoch_repeat):\n",
        "    self.env = env\n",
        "    self.policy = policy\n",
        "    self.samples_per_epoch = samples_per_epoch\n",
        "    self.epoch_repeat = epoch_repeat\n",
        "    self.obs = self.env.reset()\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def __iter__(self):\n",
        "    transitions = []\n",
        "    for step in range(self.samples_per_epoch):\n",
        "      loc, scale = self.policy(self.obs)\n",
        "      action = torch.normal(loc, scale)\n",
        "      next_obs, reward, done, info = self.env.step(action)\n",
        "      transitions.append((self.obs, loc, scale, action, reward, done, next_obs))\n",
        "      self.obs = next_obs\n",
        "\n",
        "    num_samples = self.env.num_envs * self.samples_per_epoch\n",
        "    reshape_fn = lambda x: x.view(num_samples, -1)\n",
        "    batch = map(torch.stack, zip(*transitions))\n",
        "    obs_b, loc_b, scale_b, action_b, reward_b, done_b, next_obs_b = map(reshape_fn, batch)\n",
        "\n",
        "    for repeat in range(self.epoch_repeat):\n",
        "      idx = list(range(num_samples))\n",
        "      random.shuffle(idx)\n",
        "\n",
        "      for i in idx:\n",
        "        yield obs_b[i], loc_b[i], scale_b[i], action_b[i], reward_b[i], done_b[i], next_obs_b[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jndaQdW1Oklk"
      },
      "source": [
        "#### Create the PPO algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "by58vmNAV_lP"
      },
      "outputs": [],
      "source": [
        "class PPO(LightningModule):\n",
        "\n",
        "  def __init__(self, env_name, num_envs=2048, episode_length=1000,\n",
        "               batch_size=1024, hidden_size=256, samples_per_epoch=5,\n",
        "               epoch_repeat=8, policy_lr=1e-4, value_lr=1e-3, gamma=0.97,\n",
        "               epsilon=0.3, entropy_coef=0.1, optim=AdamW):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.env = create_env(env_name, num_envs=num_envs, episode_length=episode_length)\n",
        "    test_env = gym.make(env_name, episode_length=episode_length)\n",
        "    test_env = to_torch.JaxToTorchWrapper(test_env, device=device)\n",
        "    self.test_env = NormalizeObservation(test_env)\n",
        "    self.test_env.obs_rms = self.env.obs_rms\n",
        "\n",
        "    obs_size = self.env.observation_space.shape[1]\n",
        "    action_dims = self.env.action_space.shape[1]\n",
        "\n",
        "    self.policy = GradientPolicy(obs_size, action_dims, hidden_size)\n",
        "    self.value_net = ValueNet(obs_size, hidden_size)\n",
        "    self.target_value_net = copy.deepcopy(self.value_net)\n",
        "\n",
        "    self.dataset = RLDataset(self.env, self.policy, samples_per_epoch, epoch_repeat)\n",
        "\n",
        "    self.save_hyperparameters()\n",
        "    self.videos = []\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    value_opt = self.hparams.optim(self.value_net.parameters(), lr=self.hparams.value_lr)\n",
        "    policy_opt = self.hparams.optim(self.policy.parameters(), lr=self.hparams.policy_lr)\n",
        "    return value_opt, policy_opt\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(dataset=self.dataset, batch_size=self.hparams.batch_size)\n",
        "\n",
        "  def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "    obs_b, loc_b, scale_b, action_b, reward_b, done_b, next_obs_b = batch\n",
        "\n",
        "    state_values = self.value_net(obs_b)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      next_state_values = self.target_value_net(next_obs_b)\n",
        "      next_state_values[done_b.bool()] = 0.0\n",
        "      target = reward_b + self.hparams.gamma * next_state_values\n",
        "\n",
        "    if optimizer_idx == 0:\n",
        "      loss = F.smooth_l1_loss(state_values, target)\n",
        "      self.log(\"episode/Value Loss\", loss)\n",
        "      return loss\n",
        "\n",
        "    elif optimizer_idx == 1:\n",
        "      advantages = (target - state_values).detach()\n",
        "\n",
        "      new_loc, new_scale = self.policy(obs_b)\n",
        "      dist = Normal(new_loc, new_scale)\n",
        "      log_prob = dist.log_prob(action_b).sum(dim=-1, keepdim=True)\n",
        "\n",
        "      prev_dist = Normal(loc_b, scale_b)\n",
        "      prev_log_prob = prev_dist.log_prob(action_b).sum(dim=-1, keepdim=True)\n",
        "\n",
        "      rho = torch.exp(log_prob - prev_log_prob)\n",
        "\n",
        "      surrogate_1 = rho * advantages\n",
        "      surrogate_2 = rho.clip(1 - self.hparams.epsilon, 1 + self.hparams.epsilon) * advantages\n",
        "\n",
        "      policy_loss = - torch.minimum(surrogate_1, surrogate_2)\n",
        "      entropy = dist.entropy().sum(dim=-1, keepdim=True)\n",
        "      loss = policy_loss - self.hparams.entropy_coef * entropy\n",
        "\n",
        "      self.log(\"episode/Policy Loss\", policy_loss.mean())\n",
        "      self.log(\"episode/Entropy\", entropy.mean())\n",
        "      self.log(\"episode/Reward\", reward_b.mean())\n",
        "      return loss.mean()\n",
        "\n",
        "  def training_epoch_end(self, training_epoch_outputs):\n",
        "    self.target_value_net.load_state_dict(self.value_net.state_dict())\n",
        "\n",
        "    if self.current_epoch % 10 == 0:\n",
        "      average_return = test_agent(self.test_env, self.hparams.episode_length, self.policy, episodes=1)\n",
        "      self.log(\"episode/Average Return\", average_return)\n",
        "\n",
        "    if self.current_epoch % 50 == 0:\n",
        "      video = create_video(self.test_env, self.hparams.episode_length, policy=self.policy)\n",
        "      self.videos.append(video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeohpzJXRdqr"
      },
      "source": [
        "#### Purge logs and run the visualization tool (Tensorboard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoZw9WYCTB8C"
      },
      "outputs": [],
      "source": [
        "# Start tensorboard.\n",
        "!rm -r /content/lightning_logs/\n",
        "!rm -r /content/videos/\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/lightning_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zI1Z2TVRh5m"
      },
      "source": [
        "#### Train the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r37uP7fJTB-n"
      },
      "outputs": [],
      "source": [
        "algo = PPO(\"brax-ant-v0\")\n",
        "\n",
        "trainer = Trainer(gpus=num_gpus, max_epochs=500)\n",
        "\n",
        "trainer.fit(algo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npU-_yxCjauw"
      },
      "outputs": [],
      "source": [
        "len(algo.videos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hgq57kLXAp4L"
      },
      "outputs": [],
      "source": [
        "algo.videos[9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTDeEeNxAp7M"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ZOSJl-X7zvs4",
        "Cz8DLleGz_TF",
        "CSsBrSxYIkh8",
        "UTOQuwEzIolu"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}