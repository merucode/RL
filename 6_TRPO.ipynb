{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/merucode/RL/blob/32-Study-Udemy-Policy_gradient/6_TRPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1fDTR8FNJVX"
      },
      "source": [
        "# Trust Region Policy Optimization (TRPO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uvj2rXR2f1Kw"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y xvfb\n",
        "\n",
        "!pip install gym==0.23.1 \\\n",
        "    pytorch-lightning==1.6 \\\n",
        "    pyvirtualdisplay\n",
        "\n",
        "!pip install -U brax==0.0.12 jax==0.3.14 jaxlib==0.3.14+cuda11.cudnn82 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOSJl-X7zvs4"
      },
      "source": [
        "#### Setup virtual display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-Z6takfzqGk"
      },
      "outputs": [],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display(visible=False, size=(1400, 900)).start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz8DLleGz_TF"
      },
      "source": [
        "#### Import the necessary code libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP5t6U7-nYoc"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import gym\n",
        "import matplotlib\n",
        "import functools\n",
        "import itertools\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import IterableDataset\n",
        "from torch.optim import AdamW, Optimizer\n",
        "\n",
        "from torch.distributions import Normal, kl_divergence\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "\n",
        "import brax\n",
        "from brax import envs\n",
        "from brax.envs import to_torch\n",
        "from brax.io import html\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "num_gpus = torch.cuda.device_count()\n",
        "\n",
        "v = torch.ones(1, device='cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLpKsS8qPzsN"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def create_video(env, episode_length, policy=None):\n",
        "  qp_array = []\n",
        "  state = env.reset()\n",
        "  for i in range(episode_length):\n",
        "    if policy:\n",
        "      loc, scale = policy(state)\n",
        "      sample = torch.normal(loc, scale)\n",
        "      action = torch.tanh(sample)\n",
        "    else:\n",
        "      action = env.action_space.sample()\n",
        "    state, _, _, _ = env.step(action)\n",
        "    qp_array.append(env.unwrapped._state.qp)\n",
        "  return HTML(html.render(env.unwrapped._env.sys, qp_array))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_agent(env, episode_length, policy, episodes=10):\n",
        "\n",
        "  ep_returns = []\n",
        "  for ep in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    ep_ret = 0.0\n",
        "\n",
        "    while not done:\n",
        "      loc, scale = policy(state)\n",
        "      sample = torch.normal(loc, scale)\n",
        "      action = torch.tanh(sample)\n",
        "      state, reward, done, info = env.step(action)\n",
        "      ep_ret += reward.item()\n",
        "\n",
        "    ep_returns.append(ep_ret)\n",
        "\n",
        "  return sum(ep_returns) / episodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSsBrSxYIkh8"
      },
      "source": [
        "#### Create the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hht5L2bjiNw"
      },
      "outputs": [],
      "source": [
        "class GradientPolicy(nn.Module):\n",
        "\n",
        "  def __init__(self, in_features, out_dims, hidden_size=128):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(in_features, hidden_size)\n",
        "    self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.fc_mu = nn.Linear(hidden_size, out_dims)\n",
        "    self.fc_std = nn.Linear(hidden_size, out_dims)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    loc = self.fc_mu(x)\n",
        "    loc = torch.tanh(loc)\n",
        "    scale = self.fc_std(x)\n",
        "    scale = F.softplus(scale) + 0.001\n",
        "    return loc, scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTOQuwEzIolu"
      },
      "source": [
        "#### Create the value network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtDyLSLCjiP-"
      },
      "outputs": [],
      "source": [
        "class ValueNet(nn.Module):\n",
        "\n",
        "  def __init__(self, in_features, hidden_size=128):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(in_features, hidden_size)\n",
        "    self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.fc3 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2EMZSjMIxgr"
      },
      "source": [
        "#### Create the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-AI3Jbkr4sA"
      },
      "outputs": [],
      "source": [
        "class RunningMeanStd:\n",
        "    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n",
        "    def __init__(self, epsilon=1e-4, shape=()):\n",
        "        self.mean = torch.zeros(shape, dtype=torch.float32).to(device)\n",
        "        self.var = torch.ones(shape, dtype=torch.float32).to(device)\n",
        "        self.count = epsilon\n",
        "\n",
        "    def update(self, x):\n",
        "        batch_mean = torch.mean(x, dim=0)\n",
        "        batch_var = torch.var(x, dim=0)\n",
        "        batch_count = x.shape[0]\n",
        "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
        "\n",
        "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
        "        self.mean, self.var, self.count = update_mean_var_count_from_moments(\n",
        "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count\n",
        "        )\n",
        "\n",
        "\n",
        "def update_mean_var_count_from_moments(\n",
        "    mean, var, count, batch_mean, batch_var, batch_count\n",
        "):\n",
        "    delta = batch_mean - mean\n",
        "    tot_count = count + batch_count\n",
        "\n",
        "    new_mean = mean + delta * batch_count / tot_count\n",
        "    m_a = var * count\n",
        "    m_b = batch_var * batch_count\n",
        "    M2 = m_a + m_b + torch.square(delta) * count * batch_count / tot_count\n",
        "    new_var = M2 / tot_count\n",
        "    new_count = tot_count\n",
        "\n",
        "    return new_mean, new_var, new_count\n",
        "\n",
        "\n",
        "class NormalizeObservation(gym.core.Wrapper):\n",
        "\n",
        "    def __init__(self, env, epsilon=1e-8):\n",
        "        super().__init__(env)\n",
        "        self.num_envs = getattr(env, \"num_envs\", 1)\n",
        "        self.obs_rms = RunningMeanStd(shape=self.observation_space.shape[-1])\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rews, dones, infos = self.env.step(action)\n",
        "        obs = self.normalize(obs)\n",
        "        return obs, rews, dones, infos\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return_info = kwargs.get(\"return_info\", False)\n",
        "        if return_info:\n",
        "            obs, info = self.env.reset(**kwargs)\n",
        "        else:\n",
        "            obs = self.env.reset(**kwargs)\n",
        "        obs = self.normalize(obs)\n",
        "        if not return_info:\n",
        "            return obs\n",
        "        else:\n",
        "            return obs, info\n",
        "\n",
        "    def normalize(self, obs):\n",
        "        self.obs_rms.update(obs)\n",
        "        return (obs - self.obs_rms.mean) / torch.sqrt(self.obs_rms.var + self.epsilon)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpD2i_ZSHRmN"
      },
      "outputs": [],
      "source": [
        "entry_point = functools.partial(envs.create_gym_env, env_name='ant')\n",
        "gym.register('brax-ant-v0', entry_point=entry_point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX4y_05HHRpD"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"brax-ant-v0\", episode_length=1000)\n",
        "env = to_torch.JaxToTorchWrapper(env, device=device)\n",
        "create_video(env, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dl9rXj2Sun2"
      },
      "outputs": [],
      "source": [
        "def create_env(env_name, num_envs=256, episode_length=1000):\n",
        "  env = gym.make(env_name, batch_size=num_envs, episode_length=episode_length)\n",
        "  env = to_torch.JaxToTorchWrapper(env, device=device)\n",
        "  env = NormalizeObservation(env)\n",
        "  return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEhh8tbITjvt"
      },
      "outputs": [],
      "source": [
        "env = create_env('brax-ant-v0', num_envs=10)\n",
        "obs = env.reset()\n",
        "print(\"Num envs: \", obs.shape[0], \"Obs dimentions: \", obs.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WPLsdLwTjyZ"
      },
      "outputs": [],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_LlyDyuWFcX"
      },
      "outputs": [],
      "source": [
        "obs, reward, done, info = env.step(env.action_space.sample())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vl4GvpJ4Yvl9"
      },
      "outputs": [],
      "source": [
        "info.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8qztUX_vhuE"
      },
      "source": [
        "#### TRPO optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "255iUuSLv3Zm"
      },
      "outputs": [],
      "source": [
        "# Adapted from: https://github.com/rlworkgroup/garage/blob/master/src/garage/torch/optimizers/conjugate_gradient_optimizer.py\n",
        "\n",
        "\n",
        "# Copyright (c) 2019 Reinforcement Learning Working Group\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in all\n",
        "# copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "# SOFTWARE.\n",
        "\n",
        "\n",
        "def unflatten_tensors(flattened, tensor_shapes):\n",
        "  flattened = flattened.cpu()\n",
        "  tensor_sizes = list(map(np.prod, tensor_shapes))\n",
        "  indices = np.cumsum(tensor_sizes)[:-1]\n",
        "  return [\n",
        "      np.reshape(pair[0], pair[1]).to(device)\n",
        "      for pair in zip(np.split(flattened, indices), tensor_shapes)\n",
        "  ]\n",
        "\n",
        "\n",
        "def _build_hessian_vector_product(func, params, reg_coeff=1e-5):\n",
        "    param_shapes = [p.shape or torch.Size([1]) for p in params]\n",
        "    f = func()\n",
        "    f_grads = torch.autograd.grad(f, params, create_graph=True)\n",
        "\n",
        "    def _eval(vector):\n",
        "      unflatten_vector = unflatten_tensors(vector, param_shapes)\n",
        "\n",
        "\n",
        "      assert len(f_grads) == len(unflatten_vector)\n",
        "      grad_vector_product = torch.sum(\n",
        "          torch.stack(\n",
        "              [torch.sum(g * x) for g, x in zip(f_grads, unflatten_vector)]))\n",
        "\n",
        "      hvp = list(\n",
        "          torch.autograd.grad(grad_vector_product, params,\n",
        "                              retain_graph=True))\n",
        "      for i, (hx, p) in enumerate(zip(hvp, params)):\n",
        "          if hx is None:\n",
        "              hvp[i] = torch.zeros_like(p)\n",
        "\n",
        "      flat_output = torch.cat([h.reshape(-1) for h in hvp])\n",
        "      return flat_output + reg_coeff * vector\n",
        "\n",
        "    return _eval\n",
        "\n",
        "\n",
        "def _conjugate_gradient(f_Ax, b, cg_iters, residual_tol=1e-10):\n",
        "    p = b.clone()\n",
        "    r = b.clone()\n",
        "    x = torch.zeros_like(b)\n",
        "    rdotr = torch.dot(r, r)\n",
        "\n",
        "    for _ in range(cg_iters):\n",
        "        z = f_Ax(p)\n",
        "        v = rdotr / torch.dot(p, z)\n",
        "        x += v * p\n",
        "        r -= v * z\n",
        "        newrdotr = torch.dot(r, r)\n",
        "        mu = newrdotr / rdotr\n",
        "        p = r + mu * p\n",
        "\n",
        "        rdotr = newrdotr\n",
        "        if rdotr < residual_tol:\n",
        "            break\n",
        "    return x\n",
        "\n",
        "\n",
        "class ConjugateGradientOptimizer(Optimizer):\n",
        "\n",
        "    def __init__(self, params, max_constraint_value, cg_iters=10, max_backtracks=15,\n",
        "                 backtrack_ratio=0.8, hvp_reg_coeff=1e-5, accept_violation=False):\n",
        "\n",
        "        super().__init__(params, {})\n",
        "        self._max_constraint_value = max_constraint_value\n",
        "        self._cg_iters = cg_iters\n",
        "        self._max_backtracks = max_backtracks\n",
        "        self._backtrack_ratio = backtrack_ratio\n",
        "        self._hvp_reg_coeff = hvp_reg_coeff\n",
        "        self._accept_violation = accept_violation\n",
        "\n",
        "\n",
        "    def step(self, closure):\n",
        "      f_loss, f_constraint = closure()\n",
        "      params = []\n",
        "      grads = []\n",
        "      for group in self.param_groups:\n",
        "          for p in group['params']:\n",
        "              if p.grad is not None:\n",
        "                  params.append(p)\n",
        "                  grads.append(p.grad.reshape(-1))\n",
        "\n",
        "      flat_loss_grads = torch.cat(grads)\n",
        "      f_Ax = _build_hessian_vector_product(f_constraint, params, self._hvp_reg_coeff)\n",
        "      step_dir = _conjugate_gradient(f_Ax, flat_loss_grads, self._cg_iters)\n",
        "\n",
        "      step_dir[step_dir.ne(step_dir)] = 0.\n",
        "\n",
        "      step_size = np.sqrt(2.0 * self._max_constraint_value * (1. / (torch.dot(step_dir, f_Ax(step_dir)) + 1e-8)).cpu())\n",
        "\n",
        "      if np.isnan(step_size):\n",
        "          step_size = 1.\n",
        "\n",
        "      descent_step = step_size * step_dir\n",
        "      self._backtracking_line_search(params, descent_step, f_loss, f_constraint)\n",
        "\n",
        "\n",
        "    def _backtracking_line_search(self, params, descent_step, f_loss, f_constraint):\n",
        "        prev_params = [p.clone() for p in params]\n",
        "        ratio_list = self._backtrack_ratio**np.arange(self._max_backtracks)\n",
        "        loss_before = f_loss()\n",
        "\n",
        "        param_shapes = [p.shape or torch.Size([1]) for p in params]\n",
        "        descent_step = unflatten_tensors(descent_step, param_shapes)\n",
        "        assert len(descent_step) == len(params)\n",
        "\n",
        "        for ratio in ratio_list:\n",
        "            for step, prev_param, param in zip(descent_step, prev_params, params):\n",
        "                step = ratio * step\n",
        "                new_param = prev_param.data - step\n",
        "                param.data = new_param.data\n",
        "\n",
        "            loss = f_loss()\n",
        "            constraint_val = f_constraint()\n",
        "            if (loss < loss_before and constraint_val <= self._max_constraint_value):\n",
        "                break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hI3qJPwI4Xo"
      },
      "source": [
        "#### Create the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEAvdPQBb5Pz"
      },
      "outputs": [],
      "source": [
        "class RLDataset(IterableDataset):\n",
        "\n",
        "  def __init__(self, env, policy, value_net, samples_per_epoch, gamma, lamb):\n",
        "\n",
        "    self.samples_per_epoch = samples_per_epoch\n",
        "    self.gamma = gamma\n",
        "    self.lamb = lamb\n",
        "    self.env = env\n",
        "    self.policy = policy\n",
        "    self.value_net = value_net\n",
        "    self.obs = self.env.reset()\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def __iter__(self):\n",
        "    transitions = []\n",
        "    for step in range(self.samples_per_epoch):\n",
        "      loc, scale = self.policy(self.obs)\n",
        "      action = torch.normal(loc, scale)\n",
        "      next_obs, reward, done, info = self.env.step(action)\n",
        "      transitions.append((self.obs, loc, scale, action, reward, done, next_obs))\n",
        "      self.obs = next_obs\n",
        "\n",
        "    transitions = map(torch.stack, zip(*transitions))\n",
        "    obs_b, loc_b, scale_b, action_b, reward_b, done_b, next_obs_b = transitions\n",
        "    reward_b = reward_b.unsqueeze(dim=-1)\n",
        "    done_b = done_b.unsqueeze(dim=-1)\n",
        "\n",
        "    values_b = self.value_net(obs_b)\n",
        "    next_values_b = self.value_net(next_obs_b)\n",
        "\n",
        "    td_error_b = reward_b + (1 - done_b) * self.gamma * next_values_b - values_b\n",
        "\n",
        "    running_gae = torch.zeros((self.env.num_envs, 1), dtype=torch.float32, device=device)\n",
        "    gae_b = torch.zeros_like(td_error_b)\n",
        "\n",
        "    for row in range(self.samples_per_epoch - 1, -1, -1):\n",
        "      running_gae = td_error_b[row] + (1 - done_b[row]) * self.gamma * self.lamb * running_gae\n",
        "      gae_b[row] = running_gae\n",
        "\n",
        "    target_b = gae_b + values_b\n",
        "\n",
        "    num_samples = self.samples_per_epoch * self.env.num_envs\n",
        "    reshape_fn = lambda x: x.view(num_samples, -1)\n",
        "    batch = [obs_b, loc_b, scale_b, action_b, reward_b, gae_b, target_b]\n",
        "\n",
        "    obs_b, loc_b, scale_b, action_b, reward_b, gae_b, target_b = map(reshape_fn, batch)\n",
        "\n",
        "    idx = list(range(num_samples))\n",
        "    random.shuffle(idx)\n",
        "\n",
        "    for i in idx:\n",
        "      yield obs_b[i], loc_b[i], scale_b[i], action_b[i], reward_b[i], gae_b[i], target_b[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jndaQdW1Oklk"
      },
      "source": [
        "#### Create TRPO with generalized advantage estimation (GAE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8obexPDExeAm"
      },
      "outputs": [],
      "source": [
        "class TRPO(LightningModule):\n",
        "\n",
        "  def __init__(self, env_name, num_envs=2048, episode_length=1000,\n",
        "               batch_size=2048, hidden_size=256, samples_per_epoch=20,\n",
        "               value_lr=1e-3, gamma=0.99, lamb=0.95,  kl_limit=0.25,\n",
        "               v_optim=AdamW, pi_optim=ConjugateGradientOptimizer):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.env = create_env(env_name, num_envs=num_envs, episode_length=episode_length)\n",
        "    test_env = gym.make(env_name, episode_length=episode_length)\n",
        "    test_env = to_torch.JaxToTorchWrapper(test_env, device=device)\n",
        "    self.test_env = NormalizeObservation(test_env)\n",
        "    self.test_env.obs_rms = self.env.obs_rms\n",
        "\n",
        "    obs_size = self.env.observation_space.shape[1]\n",
        "    action_dims = self.env.action_space.shape[1]\n",
        "\n",
        "    self.policy = GradientPolicy(obs_size, action_dims, hidden_size)\n",
        "    self.value_net = ValueNet(obs_size, hidden_size)\n",
        "    self.target_value_net = copy.deepcopy(self.value_net)\n",
        "\n",
        "    self.pi_optim = ConjugateGradientOptimizer(self.policy.parameters(), max_constraint_value=kl_limit)\n",
        "\n",
        "    self.dataset = RLDataset(self.env, self.policy, self.target_value_net,\n",
        "                             samples_per_epoch, gamma, lamb)\n",
        "\n",
        "    self.save_hyperparameters()\n",
        "    self.videos = []\n",
        "\n",
        "    self.automatic_optimization = False\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    value_opt = self.hparams.v_optim(self.value_net.parameters(), lr=self.hparams.value_lr)\n",
        "    pi_optim = ConjugateGradientOptimizer(self.policy.parameters(), max_constraint_value=self.hparams.kl_limit)\n",
        "    return [value_opt, pi_optim]\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(dataset=self.dataset, batch_size=self.hparams.batch_size)\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    obs_b, loc_b, scale_b, action_b, reward_b, gae_b, target_b = batch\n",
        "    v_optim, pi_optim = self.optimizers()\n",
        "\n",
        "    state_values = self.value_net(obs_b)\n",
        "    v_loss = F.smooth_l1_loss(state_values, target_b)\n",
        "    self.log(\"episode/Value Loss\", v_loss)\n",
        "\n",
        "    v_optim.zero_grad()\n",
        "    self.manual_backward(v_loss)\n",
        "    v_optim.step()\n",
        "\n",
        "    new_loc, new_scale = self.policy(obs_b)\n",
        "    dist = Normal(new_loc, new_scale)\n",
        "    log_prob = dist.log_prob(action_b).sum(dim=-1, keepdim=True)\n",
        "\n",
        "    prev_dist = Normal(loc_b, scale_b)\n",
        "    prev_log_prob = prev_dist.log_prob(action_b).sum(dim=-1, keepdim=True)\n",
        "\n",
        "    def loss_fn():\n",
        "      loss = - torch.exp(log_prob - prev_log_prob) * gae_b\n",
        "      return loss.mean()\n",
        "\n",
        "    def constraint_fn():\n",
        "      constraint = kl_divergence(prev_dist, dist).sum(dim=-1)\n",
        "      return constraint.mean()\n",
        "\n",
        "    closure = lambda: (loss_fn, constraint_fn)\n",
        "\n",
        "    loss = loss_fn()\n",
        "\n",
        "    pi_optim.zero_grad()\n",
        "    self.manual_backward(loss, retain_graph=True)\n",
        "    pi_optim.step(closure)\n",
        "\n",
        "    self.log(\"episode/Policy Loss\", loss)\n",
        "    self.log(\"episode/Reward\", reward_b.mean())\n",
        "\n",
        "\n",
        "  def training_epoch_end(self, training_step_outputs):\n",
        "    self.target_value_net.load_state_dict(self.value_net.state_dict())\n",
        "\n",
        "    if self.current_epoch % 10 == 0:\n",
        "      average_return = test_agent(self.test_env, self.hparams.episode_length, self.policy, episodes=1)\n",
        "      self.log(\"episode/Average Return\", average_return)\n",
        "\n",
        "    if self.current_epoch % 50 == 0:\n",
        "      video = create_video(self.test_env, self.hparams.episode_length, policy=self.policy)\n",
        "      self.videos.append(video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeohpzJXRdqr"
      },
      "source": [
        "#### Purge logs and run the visualization tool (Tensorboard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoZw9WYCTB8C"
      },
      "outputs": [],
      "source": [
        "# Start tensorboard.\n",
        "!rm -r /content/lightning_logs/\n",
        "!rm -r /content/videos/\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/lightning_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zI1Z2TVRh5m"
      },
      "source": [
        "#### Train the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r37uP7fJTB-n"
      },
      "outputs": [],
      "source": [
        "algo = TRPO('brax-ant-v0')\n",
        "\n",
        "trainer = Trainer(gpus=num_gpus, max_epochs=1000)\n",
        "\n",
        "trainer.fit(algo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npU-_yxCjauw"
      },
      "outputs": [],
      "source": [
        "algo.videos[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hgq57kLXAp4L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21-64r00BauV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QVwhfFmBawu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ij2uV2iXBazf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z47QHirOBa2V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aE-9iOS_tMb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}