{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/merucode/RL/blob/32-Study-Udemy-Policy_gradient/2_REINFORCE_continuous.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1fDTR8FNJVX"
      },
      "source": [
        "# REINFORCE for continuous action spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uvj2rXR2f1Kw"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y xvfb\n",
        "\n",
        "!pip install gym==0.23.1 \\\n",
        "    pytorch-lightning==1.6 \\\n",
        "    pyvirtualdisplay\n",
        "\n",
        "!pip install -U brax==0.0.12 jax==0.3.14 jaxlib==0.3.14+cuda11.cudnn82 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOSJl-X7zvs4"
      },
      "source": [
        "#### Setup virtual display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-Z6takfzqGk"
      },
      "outputs": [],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display(visible=False, size=(1400, 900)).start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz8DLleGz_TF"
      },
      "source": [
        "#### Import the necessary code libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP5t6U7-nYoc"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import gym\n",
        "import matplotlib\n",
        "import functools\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import IterableDataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from torch.distributions import Normal\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "\n",
        "import brax\n",
        "from brax import envs\n",
        "from brax.envs import to_torch\n",
        "from brax.io import html\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "num_gpus = torch.cuda.device_count()\n",
        "\n",
        "v = torch.ones(1, device='cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLpKsS8qPzsN"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def plot_policy(policy):\n",
        "  pos = np.linspace(-4.8, 4.8, 100)\n",
        "  vel = np.random.random(size=(10000, 1)) * 0.1\n",
        "  ang = np.linspace(-0.418, 0.418, 100)\n",
        "  ang_vel = np.random.random(size=(10000, 1)) * 0.1\n",
        "\n",
        "  g1, g2 = np.meshgrid(pos, ang)\n",
        "  grid = np.stack((g1,g2), axis=-1)\n",
        "  grid = grid.reshape(-1, 2)\n",
        "  grid = np.hstack((grid, vel, ang_vel))\n",
        "\n",
        "  grid = torch.from_numpy(grid).float()\n",
        "  loc, _ = policy(grid)\n",
        "\n",
        "  plot_vals = loc.numpy()\n",
        "  plot_vals = plot_vals.reshape(100, 100)[::-1]\n",
        "\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  plt.imshow(plot_vals, cmap='coolwarm')\n",
        "  plt.colorbar()\n",
        "  plt.clim(-1, 1)\n",
        "  plt.title(\"P(left | s)\", size=20)\n",
        "  plt.xlabel(\"Cart Position\", size=14)\n",
        "  plt.ylabel(\"Pole angle\", size=14)\n",
        "  plt.xticks(ticks=[0, 50, 100], labels=['-4.8', '0', '4.8'])\n",
        "  plt.yticks(ticks=[100, 50, 0], labels=['-0.418', '0', '0.418'])\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def create_video(env, episode_length, policy=None):\n",
        "  qp_array = []\n",
        "  state = env.reset()\n",
        "  for i in range(episode_length):\n",
        "    if policy:\n",
        "      loc, scale = policy(state)\n",
        "      sample = torch.normal(loc, scale)\n",
        "      action = torch.tanh(sample)\n",
        "    else:\n",
        "      action = env.action_space.sample()\n",
        "    state, _, _, _ = env.step(action)\n",
        "    qp_array.append(env.unwrapped._state.qp)\n",
        "  return HTML(html.render(env.unwrapped._env.sys, qp_array))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_agent(env, episode_length, policy, episodes=10):\n",
        "  ep_returns = []\n",
        "  for ep in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    ep_ret = 0.0\n",
        "\n",
        "    while not done:\n",
        "      loc, scale = policy(state)\n",
        "      sample = torch.normal(loc, scale)\n",
        "      action = torch.tanh(sample)\n",
        "      state, reward, done, info = env.step(action)\n",
        "      ep_ret += reward.item()\n",
        "\n",
        "    ep_returns.append(ep_ret)\n",
        "\n",
        "  return sum(ep_returns) / episodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSsBrSxYIkh8"
      },
      "source": [
        "#### Create the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hht5L2bjiNw"
      },
      "outputs": [],
      "source": [
        "class GradientPolicy(nn.Module):\n",
        "\n",
        "  def __init__(self, in_features, out_dims, hidden_size=128):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(in_features, hidden_size)\n",
        "    self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.fc_mu = nn.Linear(hidden_size, out_dims)\n",
        "    self.fc_std = nn.Linear(hidden_size, out_dims)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    loc = self.fc_mu(x)\n",
        "    loc = torch.tanh(loc)\n",
        "    scale = self.fc_std(x)\n",
        "    scale = F.softplus(scale) + 0.001\n",
        "    return loc, scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2EMZSjMIxgr"
      },
      "source": [
        "#### Create the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-AI3Jbkr4sA"
      },
      "outputs": [],
      "source": [
        "class RunningMeanStd:\n",
        "    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n",
        "    def __init__(self, epsilon=1e-4, shape=()):\n",
        "        self.mean = torch.zeros(shape, dtype=torch.float32).to(device)\n",
        "        self.var = torch.ones(shape, dtype=torch.float32).to(device)\n",
        "        self.count = epsilon\n",
        "\n",
        "    def update(self, x):\n",
        "        batch_mean = torch.mean(x, dim=0)\n",
        "        batch_var = torch.var(x, dim=0)\n",
        "        batch_count = x.shape[0]\n",
        "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
        "\n",
        "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
        "        self.mean, self.var, self.count = update_mean_var_count_from_moments(\n",
        "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count\n",
        "        )\n",
        "\n",
        "\n",
        "def update_mean_var_count_from_moments(\n",
        "    mean, var, count, batch_mean, batch_var, batch_count\n",
        "):\n",
        "    delta = batch_mean - mean\n",
        "    tot_count = count + batch_count\n",
        "\n",
        "    new_mean = mean + delta * batch_count / tot_count\n",
        "    m_a = var * count\n",
        "    m_b = batch_var * batch_count\n",
        "    M2 = m_a + m_b + torch.square(delta) * count * batch_count / tot_count\n",
        "    new_var = M2 / tot_count\n",
        "    new_count = tot_count\n",
        "\n",
        "    return new_mean, new_var, new_count\n",
        "\n",
        "\n",
        "class NormalizeObservation(gym.core.Wrapper):\n",
        "\n",
        "    def __init__(self, env, epsilon=1e-8):\n",
        "        super().__init__(env)\n",
        "        self.num_envs = getattr(env, \"num_envs\", 1)\n",
        "        self.obs_rms = RunningMeanStd(shape=self.observation_space.shape[-1])\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rews, dones, infos = self.env.step(action)\n",
        "        obs = self.normalize(obs)\n",
        "        return obs, rews, dones, infos\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return_info = kwargs.get(\"return_info\", False)\n",
        "        if return_info:\n",
        "            obs, info = self.env.reset(**kwargs)\n",
        "        else:\n",
        "            obs = self.env.reset(**kwargs)\n",
        "        obs = self.normalize(obs)\n",
        "        if not return_info:\n",
        "            return obs\n",
        "        else:\n",
        "            return obs, info\n",
        "\n",
        "    def normalize(self, obs):\n",
        "        self.obs_rms.update(obs)\n",
        "        return (obs - self.obs_rms.mean) / torch.sqrt(self.obs_rms.var + self.epsilon)\n",
        "\n",
        "\n",
        "class NormalizeReward(gym.core.Wrapper):\n",
        "\n",
        "    def __init__(self, env, gamma=0.99, epsilon=1e-8):\n",
        "        super().__init__(env)\n",
        "        self.num_envs = getattr(env, \"num_envs\", 1)\n",
        "        self.return_rms = RunningMeanStd(shape=())\n",
        "        self.returns = torch.zeros(self.num_envs).to(device)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rews, dones, infos = self.env.step(action)\n",
        "        dones = dones.bool()\n",
        "        self.returns = self.returns * self.gamma + rews\n",
        "        rews = self.normalize(rews)\n",
        "        self.returns[dones] = 0.0\n",
        "        return obs, rews, dones, infos\n",
        "\n",
        "    def normalize(self, rews):\n",
        "        self.return_rms.update(self.returns)\n",
        "        return rews / torch.sqrt(self.return_rms.var + self.epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpD2i_ZSHRmN"
      },
      "outputs": [],
      "source": [
        "entry_point = functools.partial(envs.create_gym_env, env_name='inverted_pendulum')\n",
        "gym.register('brax-inverted_pendulum-v0', entry_point=entry_point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX4y_05HHRpD"
      },
      "outputs": [],
      "source": [
        "def create_env(env_name, num_envs=256, episode_length=1000):\n",
        "  env = gym.make(env_name, batch_size=num_envs, episode_length=episode_length)\n",
        "  env = to_torch.JaxToTorchWrapper(env, device=device)\n",
        "  env = NormalizeObservation(env)\n",
        "  env = NormalizeReward(env)\n",
        "  return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEVW69uEcjBA"
      },
      "outputs": [],
      "source": [
        "env = gym.make('brax-inverted_pendulum-v0', episode_length=1000)\n",
        "env = to_torch.JaxToTorchWrapper(env, device=device)\n",
        "create_video(env, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEhh8tbITjvt"
      },
      "outputs": [],
      "source": [
        "env = create_env('brax-inverted_pendulum-v0', num_envs=1)\n",
        "obs = env.reset()\n",
        "print(\"Num envs: \", obs.shape[0], \"Obs dimentions: \", obs.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IADKlcHhhaPj"
      },
      "outputs": [],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WPLsdLwTjyZ"
      },
      "outputs": [],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_LlyDyuWFcX"
      },
      "outputs": [],
      "source": [
        "obs, reward, done, info = env.step(env.action_space.sample())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vl4GvpJ4Yvl9"
      },
      "outputs": [],
      "source": [
        "info.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i81fuTi2dCqW"
      },
      "source": [
        "#### Plot the untrained policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7viOy02c3pc"
      },
      "outputs": [],
      "source": [
        "policy = GradientPolicy(4, 1)\n",
        "grid = plot_policy(policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hI3qJPwI4Xo"
      },
      "source": [
        "#### Create the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEJnLgAnV_iR"
      },
      "outputs": [],
      "source": [
        "class RLDataset(IterableDataset):\n",
        "\n",
        "  def __init__(self, env, policy, episode_length, gamma):\n",
        "    self.env = env\n",
        "    self.policy = policy\n",
        "    self.episode_length = episode_length\n",
        "    self.gamma = gamma\n",
        "    self.obs = self.env.reset()\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def __iter__(self):\n",
        "    transitions = []\n",
        "\n",
        "    for step in range(self.episode_length):\n",
        "      loc, scale = self.policy(self.obs)\n",
        "      action = torch.normal(loc, scale)\n",
        "      next_obs, reward, done, info = self.env.step(action)\n",
        "      transitions.append((self.obs, action, reward, done))\n",
        "      self.obs = next_obs\n",
        "\n",
        "    obs_b, action_b, reward_b, done_b = map(torch.stack, zip(*transitions))\n",
        "\n",
        "    running_return = torch.zeros(self.env.num_envs, dtype=torch.float32, device=device)\n",
        "    return_b = torch.zeros_like(reward_b)\n",
        "\n",
        "    for row in range(self.episode_length - 1, -1, -1):\n",
        "      running_return = reward_b[row] + ~done_b[row] * self.gamma * running_return\n",
        "      return_b[row] = running_return\n",
        "\n",
        "    num_samples = self.env.num_envs * self.episode_length\n",
        "    obs_b = obs_b.view(num_samples, -1)\n",
        "    action_b = action_b.view(num_samples, -1)\n",
        "    return_b = return_b.view(num_samples, -1)\n",
        "\n",
        "    idx = list(range(num_samples))\n",
        "    random.shuffle(idx)\n",
        "\n",
        "    for i in idx:\n",
        "      yield obs_b[i], action_b[i], return_b[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jndaQdW1Oklk"
      },
      "source": [
        "#### Create the Proximal Policy Optimization algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "by58vmNAV_lP"
      },
      "outputs": [],
      "source": [
        "class reinforce(LightningModule):\n",
        "\n",
        "  def __init__(self, env_name, num_envs=256, episode_length=1_000, batch_size=1024,\n",
        "               hidden_size=64, policy_lr=1e-4, gamma=0.999, entropy_coef=0.0001, optim=AdamW):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.env = create_env(env_name, num_envs=num_envs, episode_length=episode_length)\n",
        "    test_env = gym.make(env_name, episode_length=episode_length)\n",
        "    test_env = to_torch.JaxToTorchWrapper(test_env, device=device)\n",
        "    self.test_env = NormalizeObservation(test_env)\n",
        "    self.test_env.obs_rms = self.env.obs_rms\n",
        "\n",
        "    obs_size = self.env.observation_space.shape[1]\n",
        "    action_dims = self.env.action_space.shape[1]\n",
        "\n",
        "    self.policy = GradientPolicy(obs_size, action_dims, hidden_size)\n",
        "    self.dataset = RLDataset(self.env, self.policy, episode_length, gamma)\n",
        "\n",
        "    self.save_hyperparameters()\n",
        "    self.videos = []\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    return self.hparams.optim(self.policy.parameters(), lr=self.hparams.policy_lr)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(dataset=self.dataset, batch_size=self.hparams.batch_size)\n",
        "\n",
        "  # Training step.\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    obs, action, returns = batch\n",
        "\n",
        "    loc, scale = self.policy(obs)\n",
        "    dist = Normal(loc, scale)\n",
        "    log_prob = dist.log_prob(action).sum(dim=-1, keepdim=True)\n",
        "\n",
        "    policy_loss = - log_prob * returns\n",
        "\n",
        "    entropy = dist.entropy().sum(dim=-1, keepdim=True)\n",
        "\n",
        "    self.log(\"episode/Policy Loss\", policy_loss.mean())\n",
        "    self.log(\"episode/Entropy\", entropy.mean())\n",
        "\n",
        "    return torch.mean(policy_loss - self.hparams.entropy_coef * entropy)\n",
        "\n",
        "  def training_epoch_end(self, training_step_outputs):\n",
        "    if self.current_epoch % 10 == 0:\n",
        "      average_return = test_agent(self.test_env, self.hparams.episode_length, self.policy, episodes=1)\n",
        "      self.log(\"episode/Average Return\", average_return)\n",
        "\n",
        "    if self.current_epoch % 50 == 0:\n",
        "      video = create_video(self.test_env, self.hparams.episode_length, policy=self.policy)\n",
        "      self.videos.append(video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeohpzJXRdqr"
      },
      "source": [
        "#### Purge logs and run the visualization tool (Tensorboard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoZw9WYCTB8C"
      },
      "outputs": [],
      "source": [
        "# Start tensorboard.\n",
        "!rm -r /content/lightning_logs/\n",
        "!rm -r /content/videos/\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/lightning_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zI1Z2TVRh5m"
      },
      "source": [
        "#### Train the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r37uP7fJTB-n"
      },
      "outputs": [],
      "source": [
        "algo = reinforce('brax-inverted_pendulum-v0')\n",
        "\n",
        "trainer = Trainer(gpus=num_gpus, max_epochs=101, log_every_n_steps=1)\n",
        "\n",
        "trainer.fit(algo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HewigwoYIWP9"
      },
      "outputs": [],
      "source": [
        "algo.videos[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kV3WzAKdIWec"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZ2qxZNUIWmB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}