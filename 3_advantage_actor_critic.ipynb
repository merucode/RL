{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/merucode/RL/blob/32-Study-Udemy-Policy_gradient/3_advantage_actor_critic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCxxWBZioi0N"
      },
      "source": [
        "# Advantage Actor-Critic\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb\n",
        "\n",
        "!pip install \\\n",
        "  gymnasium==0.26.3 \\\n",
        "  pygame \\\n",
        "  pytorch-lightning==1.6 \\\n",
        "  pyvirtualdisplay"
      ],
      "metadata": {
        "id": "oyQ7ov4M8pYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOSJl-X7zvs4"
      },
      "source": [
        "#### Setup virtual display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-Z6takfzqGk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['SDL_VIDEODRIVER']='dummy'\n",
        "import pygame\n",
        "pygame.display.set_mode((640,480))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz8DLleGz_TF"
      },
      "source": [
        "#### Import the necessary code libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP5t6U7-nYoc"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import gym\n",
        "import matplotlib\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import IterableDataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from torch.distributions import Normal\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics, NormalizeObservation\n",
        "\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "num_gpus = torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_IrPlU1wwPx"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test_env(env_name, policy, obs_rms, **kwargs):\n",
        "  env = gym.make(env_name, **kwargs)\n",
        "  env = RecordVideo(env, 'videos', episode_trigger=lambda e: True)\n",
        "  env = NormalizeObservation(env)\n",
        "  env.obs_rms = obs_rms\n",
        "  policy = policy.to(device)\n",
        "\n",
        "  for episode in range(10):\n",
        "    done = False\n",
        "    obs = env.reset()\n",
        "    while not done:\n",
        "      loc, scale = policy(obs)\n",
        "      action = torch.normal(loc, scale)\n",
        "      action = action.cpu().numpy()\n",
        "      obs, _, done, _ = env.step(action)\n",
        "  del env\n",
        "\n",
        "\n",
        "def display_video(episode=0):\n",
        "  video_file = open(f'/content/videos/rl-video-episode-{episode}.mp4', \"r+b\").read()\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"<video width=600 controls><source src='{video_url}'></video>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnk0wSWj0hAz"
      },
      "source": [
        "#### Create the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9a0b9cdnYtT"
      },
      "outputs": [],
      "source": [
        "class GradientPolicy(nn.Module):\n",
        "\n",
        "  def __init__(self, in_features, out_dims, hidden_size=128):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(in_features, hidden_size)\n",
        "    self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.fc_mu = nn.Linear(hidden_size, out_dims)\n",
        "    self.fc_std = nn.Linear(hidden_size, out_dims)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.tensor(x).float().to(device)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    loc = self.fc_mu(x)\n",
        "    loc = torch.tanh(loc) * 2\n",
        "    scale = self.fc_std(x)\n",
        "    scale = F.softplus(scale) + 0.001\n",
        "    return loc, scale"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ValueNet(nn.Module):\n",
        "\n",
        "  def __init__(self, in_features, hidden_size=128):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(in_features, hidden_size)\n",
        "    self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.fc3 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.tensor(x).float().to(device)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "AD7cI5y1FHUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yvDC9qF0oPr"
      },
      "source": [
        "#### Create the environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Pendulum-v1\")\n",
        "env.reset()"
      ],
      "metadata": {
        "id": "Ry7ECCTdWB5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(env.render(mode=\"rgb_array\"))"
      ],
      "metadata": {
        "id": "0Pbxh84fYK_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.vector.make(\"Pendulum-v1\", num_envs=2)"
      ],
      "metadata": {
        "id": "kjkGYTxUYLB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()"
      ],
      "metadata": {
        "id": "jD_2PVe_gS2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.observation_space, env.action_space"
      ],
      "metadata": {
        "id": "eh7UG1_LgT8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next_obs, rewards, dones, infos = env.step(env.action_space.sample())"
      ],
      "metadata": {
        "id": "7FGmxE1Gh12i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infos"
      ],
      "metadata": {
        "id": "nNixlpRxh15T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_env(env_name, num_envs):\n",
        "  env = gym.vector.make(env_name, num_envs)\n",
        "  env = RecordEpisodeStatistics(env)\n",
        "  env = NormalizeObservation(env)\n",
        "  return env"
      ],
      "metadata": {
        "id": "xzvNiamFh2p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jUYWliZXYLEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create the dataset"
      ],
      "metadata": {
        "id": "A5L81w8HxVn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RLDataset(IterableDataset):\n",
        "\n",
        "  def __init__(self, env, policy, steps_per_epoch):\n",
        "    self.env = env\n",
        "    self.policy = policy\n",
        "    self.steps_per_epoch = steps_per_epoch\n",
        "    self.obs = env.reset()\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def __iter__(self):\n",
        "    for step in range(self.steps_per_epoch):\n",
        "      loc, scale = self.policy(self.obs)\n",
        "      action = torch.normal(loc, scale)\n",
        "      action = action.cpu().numpy()\n",
        "      next_obs, reward, done, info = self.env.step(action)\n",
        "      yield self.obs, action, reward, done, next_obs\n",
        "      self.obs = next_obs"
      ],
      "metadata": {
        "id": "92TmeOeqetcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgXi6A4Z1p75"
      },
      "source": [
        "#### Create the A2C algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOmxUJ1vnY5d"
      },
      "outputs": [],
      "source": [
        "class A2C(LightningModule):\n",
        "\n",
        "  def __init__(self, env_name, num_envs=64, samples_per_epoch=8,\n",
        "               batch_size=1, hidden_size=64, policy_lr=1e-4, value_lr=1e-3,\n",
        "               gamma=0.99, entropy_coef=0.01, optim=AdamW):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.env = create_env(env_name, num_envs=num_envs)\n",
        "\n",
        "    obs_size = self.env.single_observation_space.shape[0]\n",
        "    action_dims = self.env.single_action_space.shape[0]\n",
        "\n",
        "    self.policy = GradientPolicy(obs_size, action_dims, hidden_size)\n",
        "    self.value_net = ValueNet(obs_size, hidden_size)\n",
        "    self.target_value_net = copy.deepcopy(self.value_net)\n",
        "\n",
        "    self.dataset = RLDataset(self.env, self.policy, samples_per_epoch)\n",
        "\n",
        "    self.save_hyperparameters()\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    value_opt = self.hparams.optim(self.value_net.parameters(), lr=self.hparams.value_lr)\n",
        "    policy_opt = self.hparams.optim(self.policy.parameters(), lr=self.hparams.policy_lr)\n",
        "    return value_opt, policy_opt\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(dataset=self.dataset, batch_size=self.hparams.batch_size)\n",
        "\n",
        "  def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "\n",
        "    samples, envs = batch[0].shape[0:2]\n",
        "    reshape_fn = lambda x: x.view(samples * envs, -1)\n",
        "    obs_b, action_b, reward_b, done_b, next_obs_b = map(reshape_fn, batch)\n",
        "\n",
        "    state_values = self.value_net(obs_b)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      next_state_values = self.target_value_net(next_obs_b)\n",
        "      next_state_values[done_b] = 0.0\n",
        "      target = reward_b + self.hparams.gamma * next_state_values\n",
        "\n",
        "    if optimizer_idx == 0:\n",
        "      loss = F.smooth_l1_loss(state_values, target)\n",
        "      self.log(\"episode/Value Loss\", loss)\n",
        "      return loss\n",
        "\n",
        "    elif optimizer_idx == 1:\n",
        "      advantages = (target - state_values).detach()\n",
        "\n",
        "      loc, scale = self.policy(obs_b)\n",
        "      dist = Normal(loc, scale)\n",
        "\n",
        "      log_probs = dist.log_prob(action_b).sum(dim=-1, keepdim=True)\n",
        "      entropy = dist.entropy().sum(dim=-1, keepdim=True)\n",
        "\n",
        "      pg_loss = - log_probs * advantages\n",
        "      loss = (pg_loss - self.hparams.entropy_coef * entropy).mean()\n",
        "\n",
        "      self.log(\"episode/PG Loss\", pg_loss.mean())\n",
        "      self.log(\"episode/Entropy\", entropy.mean())\n",
        "\n",
        "      return loss\n",
        "\n",
        "  def training_epoch_end(self, training_step_outputs):\n",
        "    if self.current_epoch > 0 and self.current_epoch % 25 == 0:\n",
        "      self.log(\"episode/Return\", self.env.return_queue[-1])\n",
        "\n",
        "    if self.current_epoch > 0 and self.current_epoch % 10 == 0:\n",
        "      self.target_value_net.load_state_dict(self.value_net.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mm9P0sX1wAA"
      },
      "source": [
        "#### Purge logs and run the visualization tool (Tensorboard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfGQdpn0nY99"
      },
      "outputs": [],
      "source": [
        "!rm -r /content/lightning_logs/\n",
        "!rm -r /content/videos/\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/lightning_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8GdIwla1wrW"
      },
      "source": [
        "#### Train the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig8c_RM8nZLN"
      },
      "outputs": [],
      "source": [
        "algo = A2C(\"Pendulum-v1\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    gpus=num_gpus,\n",
        "    max_epochs=2000,\n",
        "    log_every_n_steps=1\n",
        ")\n",
        "\n",
        "trainer.fit(algo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD3x39w71xWR"
      },
      "source": [
        "#### Check the resulting policy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "test_env('Pendulum-v1', algo.policy, algo.env.obs_rms)"
      ],
      "metadata": {
        "id": "YJSqX0Xj4IW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PleQkLR-yNM"
      },
      "outputs": [],
      "source": [
        "display_video(episode=5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bBQmph67MUru"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}