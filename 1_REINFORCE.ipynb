{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/merucode/RL/blob/32-Study-Udemy-Policy_gradient/1_REINFORCE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCxxWBZioi0N"
      },
      "source": [
        "# REINFORCE\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb\n",
        "\n",
        "!pip install \\\n",
        "  pygame \\\n",
        "  gym==0.23.1 \\\n",
        "  pytorch-lightning==1.6 \\\n",
        "  pyvirtualdisplay"
      ],
      "metadata": {
        "id": "oyQ7ov4M8pYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOSJl-X7zvs4"
      },
      "source": [
        "#### Setup virtual display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-Z6takfzqGk"
      },
      "outputs": [],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display(visible=False, size=(1400, 900)).start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz8DLleGz_TF"
      },
      "source": [
        "#### Import the necessary code libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP5t6U7-nYoc"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import gym\n",
        "import matplotlib\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import IterableDataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics, \\\n",
        "  NormalizeObservation, NormalizeReward\n",
        "\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "num_gpus = torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_IrPlU1wwPx"
      },
      "outputs": [],
      "source": [
        "def plot_policy(policy):\n",
        "  pos = np.linspace(-4.8, 4.8, 100)\n",
        "  vel = np.random.random(size=(10000, 1)) * 0.1\n",
        "  ang = np.linspace(-0.418, 0.418, 100)\n",
        "  ang_vel = np.random.random(size=(10000, 1)) * 0.1\n",
        "\n",
        "  g1, g2 = np.meshgrid(pos, ang)\n",
        "  grid = np.stack((g1,g2), axis=-1)\n",
        "  grid = grid.reshape(-1, 2)\n",
        "  grid = np.hstack((grid, vel, ang_vel))\n",
        "\n",
        "  probs = policy(grid).detach().numpy()\n",
        "  probs_left = probs[:, 0]\n",
        "\n",
        "  probs_left = probs_left.reshape(100, 100)\n",
        "  probs_left = np.flip(probs_left, axis=1)\n",
        "\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  plt.imshow(probs_left, cmap='coolwarm')\n",
        "  plt.colorbar()\n",
        "  plt.clim(0, 1)\n",
        "  plt.title(\"P(left | s)\", size=20)\n",
        "  plt.xlabel(\"Cart Position\", size=14)\n",
        "  plt.ylabel(\"Pole angle\", size=14)\n",
        "  plt.xticks(ticks=[0, 50, 100], labels=['-4.8', '0', '4.8'])\n",
        "  plt.yticks(ticks=[100, 50, 0], labels=['-0.418', '0', '0.418'])\n",
        "\n",
        "\n",
        "def test_env(env_name, policy, obs_rms):\n",
        "  env = gym.make(env_name)\n",
        "  env = RecordVideo(env, 'videos', episode_trigger=lambda e: True)\n",
        "  env = NormalizeObservation(env)\n",
        "  env.obs_rms = obs_rms\n",
        "\n",
        "  for episode in range(10):\n",
        "    done = False\n",
        "    obs = env.reset()\n",
        "    while not done:\n",
        "      action = policy(obs).multinomial(1).cpu().item()\n",
        "      obs, _, done, _ = env.step(action)\n",
        "  del env\n",
        "\n",
        "\n",
        "def display_video(episode=0):\n",
        "  video_file = open(f'/content/videos/rl-video-episode-{episode}.mp4', \"r+b\").read()\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"<video width=600 controls><source src='{video_url}'></video>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnk0wSWj0hAz"
      },
      "source": [
        "#### Create the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9a0b9cdnYtT"
      },
      "outputs": [],
      "source": [
        "class GradientPolicy(nn.Module):\n",
        "\n",
        "  def __init__(self, in_features, n_actions, hidden_size=128):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(in_features, hidden_size)\n",
        "    self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.fc3 = nn.Linear(hidden_size, n_actions)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.tensor(x).float().to(device)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = F.softmax(self.fc3(x), dim=-1)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot the untrained policy"
      ],
      "metadata": {
        "id": "sHcFutvhJ9Ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = GradientPolicy(4, 2)\n",
        "grid = plot_policy(policy)"
      ],
      "metadata": {
        "id": "8gsTHMaNJ8ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid"
      ],
      "metadata": {
        "id": "RavjH1SEL_4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yvDC9qF0oPr"
      },
      "source": [
        "#### Create the environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.vector.make(\"CartPole-v1\", num_envs=2)"
      ],
      "metadata": {
        "id": "Ry7ECCTdWB5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()"
      ],
      "metadata": {
        "id": "0Pbxh84fYK_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.observation_space, env.action_space"
      ],
      "metadata": {
        "id": "kjkGYTxUYLB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actions = np.array([0, 0])\n",
        "next_obs, rewards, dones, infos = env.step(actions)"
      ],
      "metadata": {
        "id": "jD_2PVe_gS2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next_obs"
      ],
      "metadata": {
        "id": "eh7UG1_LgT8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewards"
      ],
      "metadata": {
        "id": "7FGmxE1Gh12i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dones"
      ],
      "metadata": {
        "id": "nNixlpRxh15T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infos"
      ],
      "metadata": {
        "id": "xzvNiamFh2p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_env(env_name, num_envs):\n",
        "  env = gym.vector.make(env_name, num_envs=num_envs)\n",
        "  env = RecordEpisodeStatistics(env)\n",
        "  env = NormalizeObservation(env)\n",
        "  env = NormalizeReward(env)\n",
        "  return env"
      ],
      "metadata": {
        "id": "jUYWliZXYLEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create the dataset"
      ],
      "metadata": {
        "id": "A5L81w8HxVn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RLDataset(IterableDataset):\n",
        "\n",
        "  def __init__(self, env, policy, steps_per_epoch, gamma):\n",
        "    self.env = env\n",
        "    self.policy = policy\n",
        "    self.steps_per_epoch = steps_per_epoch\n",
        "    self.gamma = gamma\n",
        "    self.obs = env.reset()\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def __iter__(self):\n",
        "    transitions = []\n",
        "\n",
        "    for step in range(self.steps_per_epoch):\n",
        "      action = self.policy(self.obs)\n",
        "      action = action.multinomial(1).cpu().numpy()\n",
        "      next_obs, reward, done, info = self.env.step(action.flatten())\n",
        "      transitions.append((self.obs, action, reward, done))\n",
        "      self.obs = next_obs\n",
        "\n",
        "    obs_b, action_b, reward_b, done_b = map(np.stack, zip(*transitions))\n",
        "\n",
        "    running_return = np.zeros(self.env.num_envs, dtype=np.float32)\n",
        "    return_b = np.zeros_like(reward_b)\n",
        "\n",
        "    for row in range(self.steps_per_epoch - 1, -1, -1):\n",
        "      running_return = reward_b[row] + (1 - done_b[row]) * self.gamma * running_return\n",
        "      return_b[row] = running_return\n",
        "\n",
        "    num_samples = self.env.num_envs * self.steps_per_epoch\n",
        "    obs_b = obs_b.reshape(num_samples, -1)\n",
        "    action_b = action_b.reshape(num_samples, -1)\n",
        "    return_b = return_b.reshape(num_samples, -1)\n",
        "\n",
        "    idx = list(range(num_samples))\n",
        "    random.shuffle(idx)\n",
        "\n",
        "    for i in idx:\n",
        "      yield obs_b[i], action_b[i], return_b[i]"
      ],
      "metadata": {
        "id": "92TmeOeqetcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgXi6A4Z1p75"
      },
      "source": [
        "#### Create the REINFORCE algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOmxUJ1vnY5d"
      },
      "outputs": [],
      "source": [
        "class Reinforce(LightningModule):\n",
        "\n",
        "  def __init__(self, env_name, num_envs=8, samples_per_epoch=1000, batch_size=1024,\n",
        "               hidden_size=64, policy_lr=0.001, gamma=0.99, entropy_coef=0.001, optim=AdamW):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.env = create_env(env_name, num_envs=num_envs)\n",
        "\n",
        "    obs_size = self.env.single_observation_space.shape[0]\n",
        "    n_actions = self.env.single_action_space.n\n",
        "\n",
        "    self.policy = GradientPolicy(obs_size, n_actions, hidden_size)\n",
        "    self.dataset = RLDataset(self.env, self.policy, samples_per_epoch, gamma)\n",
        "\n",
        "    self.save_hyperparameters()\n",
        "\n",
        "  # Configure optimizers.\n",
        "  def configure_optimizers(self):\n",
        "    return self.hparams.optim(self.policy.parameters(), lr=self.hparams.policy_lr)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(dataset=self.dataset, batch_size=self.hparams.batch_size)\n",
        "\n",
        "  # Training step.\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    obs, actions, returns = batch\n",
        "\n",
        "    probs = self.policy(obs)\n",
        "    log_probs = torch.log(probs + 1e-6)\n",
        "    action_log_prob = log_probs.gather(1, actions)\n",
        "\n",
        "    entropy = - torch.sum(probs * log_probs, dim=-1, keepdim=True)\n",
        "\n",
        "    pg_loss = - action_log_prob * returns\n",
        "    loss = (pg_loss - self.hparams.entropy_coef * entropy).mean()\n",
        "\n",
        "    self.log(\"episode/PG Loss\", pg_loss.mean())\n",
        "    self.log(\"episode/Entropy\", entropy.mean())\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def training_epoch_end(self, training_step_outputs):\n",
        "    self.log(\"episode/Return\", self.env.return_queue[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mm9P0sX1wAA"
      },
      "source": [
        "#### Purge logs and run the visualization tool (Tensorboard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfGQdpn0nY99"
      },
      "outputs": [],
      "source": [
        "!rm -r /content/lightning_logs/\n",
        "!rm -r /content/videos/\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/lightning_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8GdIwla1wrW"
      },
      "source": [
        "#### Train the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig8c_RM8nZLN"
      },
      "outputs": [],
      "source": [
        "algo = Reinforce('CartPole-v1')\n",
        "\n",
        "trainer = Trainer(\n",
        "  gpus=num_gpus,\n",
        "  max_epochs=100,\n",
        "  log_every_n_steps=1\n",
        ")\n",
        "\n",
        "trainer.fit(algo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD3x39w71xWR"
      },
      "source": [
        "#### Check the resulting policy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "test_env('CartPole-v1', algo.policy, algo.env.obs_rms)"
      ],
      "metadata": {
        "id": "YJSqX0Xj4IW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PleQkLR-yNM"
      },
      "outputs": [],
      "source": [
        "display_video(episode=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot the trained policy"
      ],
      "metadata": {
        "id": "eWQ2YY7gJyLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_policy(algo.policy)"
      ],
      "metadata": {
        "id": "pPV_0MwT_NvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bBQmph67MUru"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}